{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9db231bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ee5296",
   "metadata": {},
   "source": [
    "### Constantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7f7e9d",
   "metadata": {},
   "source": [
    "**Acá se deben cambiar los paths a las carpetas docs-raw-texts y queries-raw-texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6597f502",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCS_RAW_DIRECTORY_PATH = \"./docs-raw-texts/\"\n",
    "QUERIES_DIRECTORY_PATH = \"./queries-raw-texts/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848b02a6",
   "metadata": {},
   "source": [
    "### Extracción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c611f6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_naf_document(filepath: str) -> dict:\n",
    "    \"\"\"\n",
    "    Parsea un archivo NAF y extrae el título, el contenido y el publicId.\n",
    "\n",
    "    Args:\n",
    "        filepath: La ruta al archivo NAF.\n",
    "\n",
    "    Returns:\n",
    "        Un diccionario con el título, el contenido, el publicId y el raw_text, o None si hay un error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(filepath)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # Extraer el título\n",
    "        title = root.find('.//fileDesc').get('title') if root.find('.//fileDesc') is not None else None\n",
    "\n",
    "        # Extraer el contenido (raw text)\n",
    "        content = root.find('.//raw').text if root.find('.//raw') is not None else None\n",
    "\n",
    "        # Extraer el publicId\n",
    "        public_id = root.find('.//public').get('publicId') if root.find('.//public') is not None else None\n",
    "\n",
    "        # Crear raw_text\n",
    "        raw_text = f\"{title}\\n{content}\" if title and content else content or title\n",
    "        return {'ID': public_id, 'title': title, 'content': content, 'raw_text': raw_text}\n",
    "\n",
    "    except (ET.ParseError, FileNotFoundError, AttributeError) as e:\n",
    "        print(f\"Error al parsear {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def ingest_naf_documents(directory: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ingesta documentos NAF de un directorio y crea un DataFrame de pandas.\n",
    "\n",
    "    Args:\n",
    "        directory: El directorio que contiene los archivos NAF.\n",
    "\n",
    "    Returns:\n",
    "        Un DataFrame de pandas con las columnas 'title' y 'content', o None si hay un error.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".naf\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            document_data = parse_naf_document(filepath)\n",
    "            if document_data:\n",
    "                data.append(document_data)\n",
    "\n",
    "    if not data:\n",
    "        print(\"No se encontraron archivos NAF válidos en el directorio.\")\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58cb04a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ID                                              title  \\\n",
      "0  d102  William Makepeace Thackeray’s deft Skewering o...   \n",
      "1  d035  Nicholas Culpeper and the Complete Herbs of En...   \n",
      "2  d321                    Aviation Pioneer Harriet Quimby   \n",
      "3  d094                   The Plays of George Bernard Shaw   \n",
      "4  d014  Hermann ‘Klecks’ Rorschach and his Eponymous Test   \n",
      "\n",
      "                                             content  \\\n",
      "0  William Makepeace Thackeray’s deft Skewering o...   \n",
      "1  Nicholas Culpeper and the Complete Herbs of En...   \n",
      "2  Aviation Pioneer Harriet Quimby.\\n\\nHarriet Qu...   \n",
      "3  The Plays of George Bernard Shaw.\\n\\nGeorge Be...   \n",
      "4  Hermann ‘Klecks’ Rorschach and his Eponymous T...   \n",
      "\n",
      "                                            raw_text  \n",
      "0  William Makepeace Thackeray’s deft Skewering o...  \n",
      "1  Nicholas Culpeper and the Complete Herbs of En...  \n",
      "2  Aviation Pioneer Harriet Quimby\\nAviation Pion...  \n",
      "3  The Plays of George Bernard Shaw\\nThe Plays of...  \n",
      "4  Hermann ‘Klecks’ Rorschach and his Eponymous T...  \n",
      "****************************************************************************************************\n",
      "Número de documentos cargados: 331\n"
     ]
    }
   ],
   "source": [
    "df_documents = ingest_naf_documents(DOCS_RAW_DIRECTORY_PATH)\n",
    "\n",
    "if df_documents is not None:\n",
    "    print(df_documents.head())\n",
    "    print(100*\"*\")\n",
    "    print(f\"Número de documentos cargados: {len(df_documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c78b051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>raw_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d102</td>\n",
       "      <td>William Makepeace Thackeray’s deft Skewering o...</td>\n",
       "      <td>William Makepeace Thackeray’s deft Skewering o...</td>\n",
       "      <td>William Makepeace Thackeray’s deft Skewering o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d035</td>\n",
       "      <td>Nicholas Culpeper and the Complete Herbs of En...</td>\n",
       "      <td>Nicholas Culpeper and the Complete Herbs of En...</td>\n",
       "      <td>Nicholas Culpeper and the Complete Herbs of En...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d321</td>\n",
       "      <td>Aviation Pioneer Harriet Quimby</td>\n",
       "      <td>Aviation Pioneer Harriet Quimby.\\n\\nHarriet Qu...</td>\n",
       "      <td>Aviation Pioneer Harriet Quimby\\nAviation Pion...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d094</td>\n",
       "      <td>The Plays of George Bernard Shaw</td>\n",
       "      <td>The Plays of George Bernard Shaw.\\n\\nGeorge Be...</td>\n",
       "      <td>The Plays of George Bernard Shaw\\nThe Plays of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d014</td>\n",
       "      <td>Hermann ‘Klecks’ Rorschach and his Eponymous Test</td>\n",
       "      <td>Hermann ‘Klecks’ Rorschach and his Eponymous T...</td>\n",
       "      <td>Hermann ‘Klecks’ Rorschach and his Eponymous T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>d098</td>\n",
       "      <td>Friedrich Bessel and the Distances of Stars</td>\n",
       "      <td>Friedrich Bessel and the Distances of Stars.\\n...</td>\n",
       "      <td>Friedrich Bessel and the Distances of Stars\\nF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>d043</td>\n",
       "      <td>Henry Cavendish and the Weight of the Earth</td>\n",
       "      <td>Henry Cavendish and the Weight of the Earth.\\n...</td>\n",
       "      <td>Henry Cavendish and the Weight of the Earth\\nH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>d208</td>\n",
       "      <td>ENIAC – The First Computer Introduced Into Public</td>\n",
       "      <td>ENIAC – The First Computer Introduced Into Pub...</td>\n",
       "      <td>ENIAC – The First Computer Introduced Into Pub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>d261</td>\n",
       "      <td>Andrea Cesalpino and the Classification of Plants</td>\n",
       "      <td>Andrea Cesalpino and the Classification of Pla...</td>\n",
       "      <td>Andrea Cesalpino and the Classification of Pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>d236</td>\n",
       "      <td>John Fitch and the Steam Boat</td>\n",
       "      <td>John Fitch and the Steam Boat.\\n\\nSteamboat of...</td>\n",
       "      <td>John Fitch and the Steam Boat\\nJohn Fitch and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>331 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID                                              title  \\\n",
       "0    d102  William Makepeace Thackeray’s deft Skewering o...   \n",
       "1    d035  Nicholas Culpeper and the Complete Herbs of En...   \n",
       "2    d321                    Aviation Pioneer Harriet Quimby   \n",
       "3    d094                   The Plays of George Bernard Shaw   \n",
       "4    d014  Hermann ‘Klecks’ Rorschach and his Eponymous Test   \n",
       "..    ...                                                ...   \n",
       "326  d098        Friedrich Bessel and the Distances of Stars   \n",
       "327  d043        Henry Cavendish and the Weight of the Earth   \n",
       "328  d208  ENIAC – The First Computer Introduced Into Public   \n",
       "329  d261  Andrea Cesalpino and the Classification of Plants   \n",
       "330  d236                      John Fitch and the Steam Boat   \n",
       "\n",
       "                                               content  \\\n",
       "0    William Makepeace Thackeray’s deft Skewering o...   \n",
       "1    Nicholas Culpeper and the Complete Herbs of En...   \n",
       "2    Aviation Pioneer Harriet Quimby.\\n\\nHarriet Qu...   \n",
       "3    The Plays of George Bernard Shaw.\\n\\nGeorge Be...   \n",
       "4    Hermann ‘Klecks’ Rorschach and his Eponymous T...   \n",
       "..                                                 ...   \n",
       "326  Friedrich Bessel and the Distances of Stars.\\n...   \n",
       "327  Henry Cavendish and the Weight of the Earth.\\n...   \n",
       "328  ENIAC – The First Computer Introduced Into Pub...   \n",
       "329  Andrea Cesalpino and the Classification of Pla...   \n",
       "330  John Fitch and the Steam Boat.\\n\\nSteamboat of...   \n",
       "\n",
       "                                              raw_text  \n",
       "0    William Makepeace Thackeray’s deft Skewering o...  \n",
       "1    Nicholas Culpeper and the Complete Herbs of En...  \n",
       "2    Aviation Pioneer Harriet Quimby\\nAviation Pion...  \n",
       "3    The Plays of George Bernard Shaw\\nThe Plays of...  \n",
       "4    Hermann ‘Klecks’ Rorschach and his Eponymous T...  \n",
       "..                                                 ...  \n",
       "326  Friedrich Bessel and the Distances of Stars\\nF...  \n",
       "327  Henry Cavendish and the Weight of the Earth\\nH...  \n",
       "328  ENIAC – The First Computer Introduced Into Pub...  \n",
       "329  Andrea Cesalpino and the Classification of Pla...  \n",
       "330  John Fitch and the Steam Boat\\nJohn Fitch and ...  \n",
       "\n",
       "[331 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1fc1dd",
   "metadata": {},
   "source": [
    "### Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14730c9e",
   "metadata": {},
   "source": [
    "Este pipeline de preprocesamiento transforma el texto crudo en un formato limpio y estructurado, listo para el análisis de NLP. Se implementó usando la librería *nltk*. La secuencia de pasos es la siguiente:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38abea4",
   "metadata": {},
   "source": [
    "`INPUT` → Tokenización → Eliminación de Stop Words → Eliminación de Puntuación → Stemming → Minúsculas → `OUTPUT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d786f391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/schica/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/schica/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/schica/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def preprocess_text(text: str, language:str ='english') -> list:\n",
    "    # Tokenización\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remover stopwords\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Remover signos de puntuación (solo tokens alfanuméricos)\n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "    tokens = tokenizer.tokenize(' '.join(tokens))\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    # Pasar a minúsculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba85f39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras 5 filas del DataFrame con tokens, podemos notar que se han procesado los textos:\n",
      "                                            raw_text  \\\n",
      "0  William Makepeace Thackeray’s deft Skewering o...   \n",
      "1  Nicholas Culpeper and the Complete Herbs of En...   \n",
      "2  Aviation Pioneer Harriet Quimby\\nAviation Pion...   \n",
      "3  The Plays of George Bernard Shaw\\nThe Plays of...   \n",
      "4  Hermann ‘Klecks’ Rorschach and his Eponymous T...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [william, makepeac, thackeray, deft, skewer, h...  \n",
      "1  [nichola, culpep, complet, herb, england, nich...  \n",
      "2  [aviat, pioneer, harriet, quimbi, aviat, pione...  \n",
      "3  [the, play, georg, bernard, shaw, the, play, g...  \n",
      "4  [hermann, kleck, rorschach, eponym, test, herm...  \n"
     ]
    }
   ],
   "source": [
    "df_documents['tokens'] = df_documents['raw_text'].apply(lambda x: preprocess_text(x, language='english'))\n",
    "\n",
    "print(\"Primeras 5 filas del DataFrame con tokens, podemos notar que se han procesado los textos:\")\n",
    "print(df_documents[['raw_text', 'tokens']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5007ec9c",
   "metadata": {},
   "source": [
    "### Índice invertido y Búsqueda binaria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd115ec",
   "metadata": {},
   "source": [
    "**ENUNCIADO: [10p] Cree su propia implementación del índice invertido usando los 331 documentos en el conjunto de datos.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4595c4",
   "metadata": {},
   "source": [
    "Los pasos para construir el índice invertido son descritos a continuación (Manrique, R. Clase de PLN, 2025.)\n",
    "\n",
    "- Paso 1: Preprocesamiento.\n",
    "- Paso 2 Secuencia de Tokens: Construir una secuencia de tokens asociada al documento del cual fue extraído.\n",
    "- PASO 3: Se ordena la secuencia de tokens.\n",
    "- PASO 4: múltiples entradas de un mismo termino en un mismo documento se combinan.\n",
    "- PASO 5 Indexar: Dividir en un diccionario y postings, y agregar la información de la frecuencia a\n",
    "nivel de documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd67f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inverted_index(df: pd.DataFrame , id_col:str ='ID', tokens_col:str ='tokens') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Construye un índice invertido con postings, frecuencia de documento y frecuencia de término por documento usando solo pandas.\n",
    "    Args:\n",
    "        df: DataFrame con los documentos.\n",
    "        id_col: nombre de la columna con el ID del documento.\n",
    "        tokens_col: nombre de la columna con la lista de tokens procesados.\n",
    "    Returns:\n",
    "        DataFrame con las columnas 'term', 'postings', 'doc_freq', 'term_freqs'.\n",
    "    \"\"\"\n",
    "    # Explode para tener una fila por token y documento\n",
    "    exploded = df[[id_col, tokens_col]].explode(tokens_col)\n",
    "\n",
    "    # Calcular frecuencia de término por documento\n",
    "    term_doc_freq = exploded.groupby([tokens_col, id_col]).size().reset_index(name='freq')\n",
    "\n",
    "    # Construir diccionario {doc_id: freq} para cada término\n",
    "    term_freqs = term_doc_freq.groupby(tokens_col).apply(\n",
    "        lambda g: dict(zip(g[id_col], g['freq']))\n",
    "    )\n",
    "\n",
    "    # Elimina duplicados para postings y doc_freq\n",
    "    exploded_nodup = exploded.drop_duplicates()\n",
    "\n",
    "    grouped = exploded_nodup.groupby(tokens_col)[id_col].agg(list)\n",
    "    doc_freq = exploded_nodup.groupby(tokens_col)[id_col].nunique()\n",
    "\n",
    "    index_df = pd.DataFrame({\n",
    "        'term': grouped.index,\n",
    "        'postings': grouped.values,\n",
    "        'doc_freq': doc_freq.values,\n",
    "        'term_freqs': term_freqs.values\n",
    "    })\n",
    "\n",
    "    return index_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35e1d662",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22039/3914755163.py:18: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  term_freqs = term_doc_freq.groupby(tokens_col).apply(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>postings</th>\n",
       "      <th>doc_freq</th>\n",
       "      <th>term_freqs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>[d102, d035, d116, d071, d250, d114, d156, d18...</td>\n",
       "      <td>129</td>\n",
       "      <td>{'d003': 2, 'd004': 1, 'd006': 1, 'd011': 1, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aachen</td>\n",
       "      <td>[d252, d139, d161]</td>\n",
       "      <td>3</td>\n",
       "      <td>{'d139': 1, 'd161': 2, 'd252': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aazv</td>\n",
       "      <td>[d156]</td>\n",
       "      <td>1</td>\n",
       "      <td>{'d156': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ab</td>\n",
       "      <td>[d224]</td>\n",
       "      <td>1</td>\n",
       "      <td>{'d224': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abadon</td>\n",
       "      <td>[d062]</td>\n",
       "      <td>1</td>\n",
       "      <td>{'d062': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12767</th>\n",
       "      <td>zurich</td>\n",
       "      <td>[d014, d143, d113, d112, d030, d047, d059, d21...</td>\n",
       "      <td>11</td>\n",
       "      <td>{'d014': 2, 'd030': 2, 'd047': 2, 'd059': 1, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12768</th>\n",
       "      <td>zuse</td>\n",
       "      <td>[d211, d202]</td>\n",
       "      <td>2</td>\n",
       "      <td>{'d202': 1, 'd211': 20}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12769</th>\n",
       "      <td>zwicki</td>\n",
       "      <td>[d253]</td>\n",
       "      <td>1</td>\n",
       "      <td>{'d253': 17}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12770</th>\n",
       "      <td>zworykin</td>\n",
       "      <td>[d071, d068]</td>\n",
       "      <td>2</td>\n",
       "      <td>{'d068': 1, 'd071': 4}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12771</th>\n",
       "      <td>zygmunt</td>\n",
       "      <td>[d286]</td>\n",
       "      <td>1</td>\n",
       "      <td>{'d286': 1}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12772 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           term                                           postings  doc_freq  \\\n",
       "0             a  [d102, d035, d116, d071, d250, d114, d156, d18...       129   \n",
       "1        aachen                                 [d252, d139, d161]         3   \n",
       "2          aazv                                             [d156]         1   \n",
       "3            ab                                             [d224]         1   \n",
       "4        abadon                                             [d062]         1   \n",
       "...         ...                                                ...       ...   \n",
       "12767    zurich  [d014, d143, d113, d112, d030, d047, d059, d21...        11   \n",
       "12768      zuse                                       [d211, d202]         2   \n",
       "12769    zwicki                                             [d253]         1   \n",
       "12770  zworykin                                       [d071, d068]         2   \n",
       "12771   zygmunt                                             [d286]         1   \n",
       "\n",
       "                                              term_freqs  \n",
       "0      {'d003': 2, 'd004': 1, 'd006': 1, 'd011': 1, '...  \n",
       "1                      {'d139': 1, 'd161': 2, 'd252': 1}  \n",
       "2                                            {'d156': 1}  \n",
       "3                                            {'d224': 1}  \n",
       "4                                            {'d062': 1}  \n",
       "...                                                  ...  \n",
       "12767  {'d014': 2, 'd030': 2, 'd047': 2, 'd059': 1, '...  \n",
       "12768                            {'d202': 1, 'd211': 20}  \n",
       "12769                                       {'d253': 17}  \n",
       "12770                             {'d068': 1, 'd071': 4}  \n",
       "12771                                        {'d286': 1}  \n",
       "\n",
       "[12772 rows x 4 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_index = build_inverted_index(df_documents, id_col='ID', tokens_col='tokens')\n",
    "\n",
    "## Ver df resultante\n",
    "inverted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287a499d",
   "metadata": {},
   "source": [
    "**ENUNCIADO: [10p] Cree una función que lea el índice invertido y calcule consultas booleanas mediante el algoritmo de mezcla. El algoritmo de mezcla debe ser capaz de calcular: AND, y NOT.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a211d36",
   "metadata": {},
   "source": [
    "Los pasos para construir las consultas booleanas, mediante el algoritmo de mezcla, son:\n",
    "\n",
    "- Paso 1: Preprocesamiento. Se utiliza el mismo con el que se construyó el índice invertido.\n",
    "- Paso 2: Localizar los documentos donde se encuentran cada uno de los términos de la búsqueda.\n",
    "- OPCIONAL: Se crea la lista de todos los IDs de documentos (solo necesario para NOT)\n",
    "- PASO 3: Se ejecuta la operación de mezcla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71b96e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_query(index_df: pd.DataFrame, operation: str, terms: list, all_doc_ids: list =None, language: str ='english') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Realiza consultas booleanas (AND, OR, NOT) sobre el índice invertido en formato DataFrame.\n",
    "    Preprocesa los términos de consulta igual que los documentos.\n",
    "    Args:\n",
    "        index_df: DataFrame del índice invertido con columnas 'term', 'postings', 'doc_freq'.\n",
    "        operation: 'AND', 'OR', o 'NOT'. Se incluye OR a pesar de no estar en el enunciado.\n",
    "        terms: Lista de términos a consultar (pueden ser frases).\n",
    "        all_doc_ids: Lista de todos los IDs de documentos (solo necesario para NOT).\n",
    "        language: Idioma para el preprocesamiento.\n",
    "    Returns:\n",
    "        DataFrame con los IDs de documentos que cumplen la consulta.\n",
    "    \"\"\"\n",
    "    # Preprocesar términos de consulta\n",
    "    processed_terms = []\n",
    "    for term in terms:\n",
    "        processed = preprocess_text(term, language=language)\n",
    "        processed_terms.extend(processed)\n",
    "    # Eliminar duplicados en la consulta\n",
    "    processed_terms = list(set(processed_terms))\n",
    "\n",
    "    # Recuperar postings para cada término\n",
    "    postings_lists = []\n",
    "    for term in processed_terms:\n",
    "        row = index_df[index_df['term'] == term]\n",
    "        if not row.empty:\n",
    "            postings_lists.append(set(row.iloc[0]['postings']))\n",
    "        else:\n",
    "            postings_lists.append(set())\n",
    "\n",
    "    if operation == 'AND':\n",
    "        result = set.intersection(*postings_lists) if postings_lists else set()\n",
    "    elif operation == 'OR':\n",
    "        result = set.union(*postings_lists) if postings_lists else set()\n",
    "    elif operation == 'NOT':\n",
    "        if all_doc_ids is None:\n",
    "            raise ValueError(\"Para NOT, debe proveer all_doc_ids.\")\n",
    "        result = set(all_doc_ids) - postings_lists[0]\n",
    "    else:\n",
    "        raise ValueError(\"Operación no soportada. Use 'AND', 'OR', o 'NOT'.\")\n",
    "\n",
    "    return pd.DataFrame({'ID': sorted(result)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0827428d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consulta AND  ['create search'] :      ID\n",
      "0  d065\n",
      "1  d158\n",
      "2  d177\n",
      "3  d216\n",
      "4  d227\n",
      "5  d280\n",
      "6  d314\n",
      "****************************************************************************************************\n",
      "Consulta NOT  ['love'] :        ID\n",
      "0    d001\n",
      "1    d002\n",
      "2    d003\n",
      "3    d005\n",
      "4    d006\n",
      "..    ...\n",
      "292  d327\n",
      "293  d328\n",
      "294  d329\n",
      "295  d330\n",
      "296  d331\n",
      "\n",
      "[297 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso para AND, NOT:\n",
    "all_doc_ids = df_documents['ID'].tolist()\n",
    "consulta_and = ['create search']\n",
    "consulta_not = ['love']\n",
    "\n",
    "print(\"Consulta AND\", consulta_and, \":\", boolean_query(inverted_index, 'AND', consulta_and, all_doc_ids=all_doc_ids))\n",
    "print(100*\"*\")\n",
    "print(\"Consulta NOT\", consulta_not , \":\", boolean_query(inverted_index, 'NOT', consulta_not, all_doc_ids=all_doc_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b0c616",
   "metadata": {},
   "source": [
    "**ENUNCIADO: [5p] Para cada una de las 35 consultas en el conjunto de datos, recupere los documentos utilizando consultas binarias AND (i.e. termino_1 AND termino_2 AND termino_3…). Escriba un archivo (BSII-ANDqueries_results) con los resultados siguiendo el mismo formato que \"relevance-judgments\":**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21681d5",
   "metadata": {},
   "source": [
    "#### 1. Extraer las queries usando la función ingest_naf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e6b95e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>raw_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>q01</td>\n",
       "      <td>Fabrication of music instruments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>q02</td>\n",
       "      <td>famous German poetry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>q03</td>\n",
       "      <td>Romanticism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>q04</td>\n",
       "      <td>University of Edinburgh research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>q06</td>\n",
       "      <td>bridge construction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID                          raw_text\n",
       "23  q01  Fabrication of music instruments\n",
       "29  q02              famous German poetry\n",
       "30  q03                       Romanticism\n",
       "13  q04  University of Edinburgh research\n",
       "7   q06               bridge construction"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ingest_queries(directory: str) -> pd.DataFrame:\n",
    "    df_queries = ingest_naf_documents(directory)\n",
    "    return df_queries[['ID', 'raw_text']]\n",
    "\n",
    "df_queries = ingest_queries(QUERIES_DIRECTORY_PATH)\n",
    "\n",
    "# Ordernar por ID\n",
    "df_queries = df_queries.sort_values('ID')\n",
    "# Ver las primeras queries\n",
    "df_queries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133dbe7a",
   "metadata": {},
   "source": [
    "#### 2. Recuperar documentos para cada query usando boolean_query (AND)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764e82d0",
   "metadata": {},
   "source": [
    "Se itera sobre el DataFrame de queries, para cada registro se llama la función *boolean_query*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34b79fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_doc_ids = df_documents['ID'].tolist()\n",
    "results = []\n",
    "\n",
    "for _, row in df_queries.iterrows():\n",
    "    query_id = row['ID']\n",
    "    query_text = row['raw_text']\n",
    "    docs_df = boolean_query(inverted_index, 'AND', [query_text], all_doc_ids=all_doc_ids)\n",
    "    doc_list = ','.join(docs_df['ID'].tolist())\n",
    "    results.append(f\"{query_id}\\t{doc_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c53ba99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q01\t\n",
      "q02\td291,d293\n",
      "q03\td105,d147,d152,d283,d291,d318\n",
      "q04\td286\n",
      "q06\td026,d029,d069,d257,d297,d303,d329\n",
      "q07\td004,d034\n",
      "q08\td108,d110,d117,d205,d251\n",
      "q09\td198,d205,d223\n",
      "q10\td231\n",
      "q12\t\n",
      "q13\t\n",
      "q14\t\n",
      "q16\td132,d150,d176,d184,d229,d250,d277\n",
      "q17\td121,d271\n",
      "q18\td192,d194,d203,d210\n",
      "q19\td179\n",
      "q22\t\n",
      "q23\t\n",
      "q24\td129,d221,d240,d282\n",
      "q25\t\n",
      "q26\t\n",
      "q27\t\n",
      "q28\td136,d174\n",
      "q29\td037,d046,d294\n",
      "q32\td025,d031,d090,d139,d254\n",
      "q34\t\n",
      "q36\td257,d265\n",
      "q37\td169\n",
      "q38\t\n",
      "q40\t\n",
      "q41\td150,d174\n",
      "q42\t\n",
      "q44\td029,d185\n",
      "q45\td105\n",
      "q46\td094,d133\n"
     ]
    }
   ],
   "source": [
    "# Ver resultados de boolean_query para cada query\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17228edf",
   "metadata": {},
   "source": [
    "#### 3. Escribir el archivo de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a7a03ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"resultados/BSII-ANDqueries_results.tsv\", \"w\") as f:\n",
    "    for line in results:\n",
    "        f.write(line + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
